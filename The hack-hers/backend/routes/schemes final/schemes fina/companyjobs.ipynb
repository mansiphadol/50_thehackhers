{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05dbf34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12612\\3879878486.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests_html\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# List of proxy servers. Replace with your actual proxy server list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m proxies = [\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_html'"
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "import random\n",
    "\n",
    "# List of proxy servers. Replace with your actual proxy server list.\n",
    "proxies = [\n",
    "    'http://proxy1.example.com:8080',\n",
    "    'http://proxy2.example.com:8080',\n",
    "    'http://proxy3.example.com:8080',\n",
    "    # Add more proxies as needed\n",
    "]\n",
    "\n",
    "def scrape_with_proxies(url):\n",
    "    session = HTMLSession()\n",
    "\n",
    "    # Choose a random proxy from the list for each request\n",
    "    proxy = random.choice(proxies)\n",
    "    proxies = {'http': proxy, 'https': proxy}\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, proxies=proxies)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch URL with proxy. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    keyword = input(\"Enter the job keyword: \").strip()\n",
    "    location = input(\"Enter the location: \").strip()\n",
    "\n",
    "    url = f\"https://in.indeed.com/jobs?q={keyword}&l={location}\"\n",
    "    response = scrape_with_proxies(url)\n",
    "\n",
    "    if response is not None:\n",
    "        # Process the response using BeautifulSoup or other parsing methods\n",
    "        # ...\n",
    "\n",
    "        # For example, you can use BeautifulSoup to extract data\n",
    "        from bs4 import BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Perform data extraction here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade42ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the desired job (e.g., maid): maid\n",
      "Enter the location (e.g., mumbai): mumbai\n",
      "Classes found under <div class=\"css-1dbjc4n r-ljpsdf r-ndvcnb\">:\n",
      "['alink', 'css-1dbjc4n', 'css-1dbjc4n', 'css-1dbjc4n', 'css-76zvg2', 'css-76zvg2', 'css-1dbjc4n', 'css-76zvg2', 'css-1dbjc4n', 'css-76zvg2', 'css-1dbjc4n', 'css-1dbjc4n', 'css-76zvg2', 'css-76zvg2', 'css-1dbjc4n', 'css-1dbjc4n', 'css-18t94o4', 'css-1dbjc4n', 'css-76zvg2']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_classes(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            target_div = soup.find('div', class_='css-1dbjc4n r-ljpsdf r-ndvcnb')\n",
    "            \n",
    "            if target_div:\n",
    "                classes_list = target_div.find_all(class_=True)\n",
    "                return [css_class['class'][0] for css_class in classes_list]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Failed to fetch the URL. Status code:\", response.status_code)\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input_maid = input(\"Enter the desired job (e.g., maid): \")\n",
    "    user_input_location = input(\"Enter the location (e.g., mumbai): \")\n",
    "\n",
    "    url = f\"https://www.myrocket.co/s/{user_input_maid}-jobs-in-{user_input_location}/\"\n",
    "    classes = scrape_classes(url)\n",
    "\n",
    "    if classes:\n",
    "        print(\"Classes found under <div class=\\\"css-1dbjc4n r-ljpsdf r-ndvcnb\\\">:\")\n",
    "        print(classes)\n",
    "    else:\n",
    "        print(\"No classes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dd57cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found under <div class=\"css-1dbjc4n r-ljpsdf r-ndvcnb\">:\n",
      "['alink', 'css-1dbjc4n', 'css-1dbjc4n', 'css-1dbjc4n', 'css-76zvg2', 'css-76zvg2', 'css-1dbjc4n', 'css-76zvg2', 'css-1dbjc4n', 'css-76zvg2', 'css-1dbjc4n', 'css-1dbjc4n', 'css-76zvg2', 'css-76zvg2', 'css-1dbjc4n', 'css-1dbjc4n', 'css-18t94o4', 'css-1dbjc4n', 'css-76zvg2']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_classes(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            target_div = soup.find('div', class_='css-1dbjc4n r-ljpsdf r-ndvcnb')\n",
    "            \n",
    "            if target_div:\n",
    "                classes_list = target_div.find_all(class_=True)\n",
    "                return [css_class['class'][0] for css_class in classes_list]\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Failed to fetch the URL. Status code:\", response.status_code)\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.myrocket.co/s/maid-jobs-in-mumbai/\"\n",
    "    classes = scrape_classes(url)\n",
    "\n",
    "    if classes:\n",
    "        print(\"Classes found under <div class=\\\"css-1dbjc4n r-ljpsdf r-ndvcnb\\\">:\")\n",
    "        print(classes)\n",
    "    else:\n",
    "        print(\"No classes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adbc92c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12612\\610263484.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.myrocket.co/s/maid-jobs-in-mumbai/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12612\\610263484.py\u001b[0m in \u001b[0;36mscrape_classes\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m# Scraping div with class \"alink\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mdiv2_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'alink'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m# Scraping div with class \"css-1dbjc4n r-y285pg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_classes(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-ljpsdf r-ndvcnb\"\n",
    "            div1_classes = soup.find('div', class_='css-1dbjc4n r-ljpsdf r-ndvcnb')['class']\n",
    "\n",
    "            # Scraping div with class \"alink\"\n",
    "            div2_classes = soup.find('div', class_='alink')['class']\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-y285pg\"\n",
    "            div3_classes = soup.find('div', class_='css-1dbjc4n r-y285pg')['class']\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-14lw9ot r-sqpuna r-ymttw5 r-1f1sjgu r-bnwqim\"\n",
    "            div4_classes = soup.find('div', class_='css-1dbjc4n r-14lw9ot r-sqpuna r-ymttw5 r-1f1sjgu r-bnwqim')['class']\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-1awozwy r-6koalj r-18u37iz r-1wtj0ep\"\n",
    "            div5_classes = soup.find('div', class_='css-1dbjc4n r-1awozwy r-6koalj r-18u37iz r-1wtj0ep')['class']\n",
    "\n",
    "            # Scraping div with class \"css-76zvg2 r-1t176gz r-ubezar r-majxgm r-135wba7 r-1kb76zh r-thhjwx r-1udh08x r-1ttztb7 r-1udbk01 r-1ozqkpa r-3s2u2q\"\n",
    "            div6_classes = soup.find('div', class_='css-76zvg2 r-1t176gz r-ubezar r-majxgm r-135wba7 r-1kb76zh r-thhjwx r-1udh08x r-1ttztb7 r-1udbk01 r-1ozqkpa r-3s2u2q')['class']\n",
    "\n",
    "            # Scraping div with class \"css-76zvg2 r-1p4twd r-1enofrn r-1kfrs79 r-1cwl3u0 r-1ttztb7 r-1ozqkpa\"\n",
    "            div7_classes = soup.find('div', class_='css-76zvg2 r-1p4twd r-1enofrn r-1kfrs79 r-1cwl3u0 r-1ttztb7 r-1ozqkpa')['class']\n",
    "\n",
    "            # Combining all the classes found into one list\n",
    "            all_classes = set(div1_classes + div2_classes + div3_classes + div4_classes + div5_classes + div6_classes + div7_classes)\n",
    "            return list(all_classes)\n",
    "        else:\n",
    "            print(\"Failed to fetch the URL. Status code:\", response.status_code)\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.myrocket.co/s/maid-jobs-in-mumbai/\"\n",
    "    classes = scrape_classes(url)\n",
    "\n",
    "    if classes:\n",
    "        print(\"Classes found:\")\n",
    "        for css_class in classes:\n",
    "            print(css_class)\n",
    "    else:\n",
    "        print(\"No classes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e219400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found:\n",
      "r-1kfrs79\n",
      "r-ndvcnb\n",
      "r-1udh08x\n",
      "r-1udbk01\n",
      "r-1wtj0ep\n",
      "r-1t176gz\n",
      "r-6koalj\n",
      "r-1ozqkpa\n",
      "r-ljpsdf\n",
      "r-135wba7\n",
      "r-1enofrn\n",
      "r-ymttw5\n",
      "r-1cwl3u0\n",
      "r-thhjwx\n",
      "r-3s2u2q\n",
      "r-sqpuna\n",
      "r-1ttztb7\n",
      "r-majxgm\n",
      "r-1p4twd\n",
      "r-ubezar\n",
      "r-1f1sjgu\n",
      "r-y285pg\n",
      "css-1dbjc4n\n",
      "r-18u37iz\n",
      "r-14lw9ot\n",
      "r-1awozwy\n",
      "r-bnwqim\n",
      "css-76zvg2\n",
      "r-1kb76zh\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_classes(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-ljpsdf r-ndvcnb\"\n",
    "            div1 = soup.find('div', class_='css-1dbjc4n r-ljpsdf r-ndvcnb')\n",
    "            div1_classes = div1['class'] if div1 else []\n",
    "\n",
    "            # Scraping div with class \"alink\"\n",
    "            div2 = soup.find('div', class_='alink')\n",
    "            div2_classes = div2['class'] if div2 else []\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-y285pg\"\n",
    "            div3 = soup.find('div', class_='css-1dbjc4n r-y285pg')\n",
    "            div3_classes = div3['class'] if div3 else []\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-14lw9ot r-sqpuna r-ymttw5 r-1f1sjgu r-bnwqim\"\n",
    "            div4 = soup.find('div', class_='css-1dbjc4n r-14lw9ot r-sqpuna r-ymttw5 r-1f1sjgu r-bnwqim')\n",
    "            div4_classes = div4['class'] if div4 else []\n",
    "\n",
    "            # Scraping div with class \"css-1dbjc4n r-1awozwy r-6koalj r-18u37iz r-1wtj0ep\"\n",
    "            div5 = soup.find('div', class_='css-1dbjc4n r-1awozwy r-6koalj r-18u37iz r-1wtj0ep')\n",
    "            div5_classes = div5['class'] if div5 else []\n",
    "\n",
    "            # Scraping div with class \"css-76zvg2 r-1t176gz r-ubezar r-majxgm r-135wba7 r-1kb76zh r-thhjwx r-1udh08x r-1ttztb7 r-1udbk01 r-1ozqkpa r-3s2u2q\"\n",
    "            div6 = soup.find('div', class_='css-76zvg2 r-1t176gz r-ubezar r-majxgm r-135wba7 r-1kb76zh r-thhjwx r-1udh08x r-1ttztb7 r-1udbk01 r-1ozqkpa r-3s2u2q')\n",
    "            div6_classes = div6['class'] if div6 else []\n",
    "\n",
    "            # Scraping div with class \"css-76zvg2 r-1p4twd r-1enofrn r-1kfrs79 r-1cwl3u0 r-1ttztb7 r-1ozqkpa\"\n",
    "            div7 = soup.find('div', class_='css-76zvg2 r-1p4twd r-1enofrn r-1kfrs79 r-1cwl3u0 r-1ttztb7 r-1ozqkpa')\n",
    "            div7_classes = div7['class'] if div7 else []\n",
    "\n",
    "            # Combining all the classes found into one list\n",
    "            all_classes = set(div1_classes + div2_classes + div3_classes + div4_classes + div5_classes + div6_classes + div7_classes)\n",
    "            return list(all_classes)\n",
    "        else:\n",
    "            print(\"Failed to fetch the URL. Status code:\", response.status_code)\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.myrocket.co/s/maid-jobs-in-mumbai/\"\n",
    "    classes = scrape_classes(url)\n",
    "\n",
    "    if classes:\n",
    "        print(\"Classes found:\")\n",
    "        for css_class in classes:\n",
    "            print(css_class)\n",
    "    else:\n",
    "        print(\"No classes found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d5843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
